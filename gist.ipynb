{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pyCompare\n",
    "import matplotlib\n",
    "import keras_tuner as kt\n",
    "import h5py\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, Normalizer, RobustScaler, MaxAbsScaler\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from math import sqrt\n",
    "from keras_tuner.tuners import BayesianOptimization, Hyperband\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import LeakyReLU, Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d, median_filter\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 6)})\n",
    "sns.set(style=\"whitegrid\", font_scale=2)\n",
    "\n",
    "def read_and_clean_data(data_source, drop_outliers=True):\n",
    "    if isinstance(data_source, str):\n",
    "        data = pd.read_csv(data_source)\n",
    "    elif isinstance(data_source, StringIO):\n",
    "        data = pd.read_csv(data_source)\n",
    "    else:\n",
    "        raise ValueError(\"data_source must be a file path (str) or a StringIO object.\")\n",
    "    \n",
    "    if drop_outliers:\n",
    "        data = data[data['sample'] != 'outlier']\n",
    "    \n",
    "    data.rename(columns={\"final_age\": \"Age\"}, inplace=True)\n",
    "    \n",
    "    if data.isnull().values.any():\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "\n",
    "    print(data.head())\n",
    "    print(list(data.columns[0:10]))  # ['file_name', 'sample', 'Age', 'latitude', 'length', 'gear_depth', 'gear_temp', 'wn11476.85064', 'wn11468.60577', 'wn11460.36091']\n",
    "    return data\n",
    "\n",
    "# Savitzky-Golay filter function\n",
    "def savgol_filter_func(data, window_length=17, polyorder=2, deriv=1):\n",
    "    return savgol_filter(data, window_length=window_length, polyorder=polyorder, deriv=deriv)\n",
    "\n",
    "# Moving Average filter function\n",
    "def moving_average_filter(data, size=5):\n",
    "    return uniform_filter1d(data, size=size, axis=1)\n",
    "\n",
    "# Gaussian filter function\n",
    "def gaussian_filter_func(data, sigma=2):\n",
    "    return gaussian_filter1d(data, sigma=sigma, axis=1)\n",
    "\n",
    "# Median filter function\n",
    "def median_filter_func(data, size=5):\n",
    "    return median_filter(data, size=(1, size))\n",
    "\n",
    "# Wavelet filter function\n",
    "def wavelet_filter_func(data, wavelet='db1', level=1):\n",
    "    def apply_wavelet(signal):\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "        coeffs[1:] = [pywt.threshold(i, value=0.5 * max(i)) for i in coeffs[1:]]\n",
    "        return pywt.waverec(coeffs, wavelet)\n",
    "    \n",
    "    return np.apply_along_axis(apply_wavelet, axis=1, arr=data)\n",
    "\n",
    "# Fourier filter function\n",
    "def fourier_filter_func(data, threshold=0.1):\n",
    "    def apply_fft(signal):\n",
    "        fft_data = np.fft.fft(signal)\n",
    "        frequencies = np.fft.fftfreq(len(signal))\n",
    "        fft_data[np.abs(frequencies) > threshold] = 0\n",
    "        return np.fft.ifft(fft_data).real\n",
    "    \n",
    "    return np.apply_along_axis(apply_fft, axis=1, arr=data)\n",
    "\n",
    "# PCA filter function\n",
    "def pca_filter_func(data, n_components=5):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    transformed = pca.fit_transform(data)\n",
    "    return pca.inverse_transform(transformed)\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_spectra(data, filter_type='savgol'):\n",
    "    filter_functions = {\n",
    "        'savgol': savgol_filter_func,\n",
    "        'moving_average': moving_average_filter,\n",
    "        'gaussian': gaussian_filter_func,\n",
    "        'median': median_filter_func,\n",
    "        'wavelet': wavelet_filter_func,\n",
    "        'fourier': fourier_filter_func,\n",
    "        'pca': pca_filter_func\n",
    "    }\n",
    "    \n",
    "    filter_func = filter_functions.get(filter_type, savgol_filter_func)\n",
    "    \n",
    "    data.loc[data['sample'] == 'training', data.columns[4:]] = filter_func(data.loc[data['sample'] == 'training', data.columns[4:]].values)\n",
    "    data.loc[data['sample'] == 'test', data.columns[4:]] = filter_func(data.loc[data['sample'] == 'test', data.columns[4:]].values)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def apply_normalization(data, columns):\n",
    "    normalizer = Normalizer()\n",
    "    data[columns] = normalizer.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "def apply_robust_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = RobustScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: RobustScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_minmax_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = MinMaxScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: MinMaxScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_maxabs_scaling(data, y_col, feature_columns):\n",
    "    scaler_y = MaxAbsScaler()\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    data[feature_columns] = data[feature_columns].apply(lambda col: MaxAbsScaler().fit_transform(col.values.reshape(-1, 1)))\n",
    "    return data, scaler_y\n",
    "\n",
    "def apply_scaling(data, scaling_method='standard', y_col='Age'):\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'maxabs': MaxAbsScaler(),\n",
    "        'robust': RobustScaler(),\n",
    "        'normalize': Normalizer()  \n",
    "    }\n",
    "    \n",
    "    if scaling_method not in scalers:\n",
    "        raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "    \n",
    "    feature_columns = data.columns.difference(['sample', 'file_name', y_col])\n",
    "\n",
    "    if scaling_method == 'normalize':\n",
    "        data = apply_normalization(data, feature_columns)\n",
    "        return data, None  \n",
    "    \n",
    "    scaler_y = scalers[scaling_method]\n",
    "    data[y_col] = scaler_y.fit_transform(data[[y_col]])\n",
    "    scaler_x = scalers[scaling_method]\n",
    "    data[feature_columns] = scaler_x.fit_transform(data[feature_columns])\n",
    "    \n",
    "    return data, scaler_y\n",
    "\n",
    "def build_model(hp, input_dim_A, input_dim_B):\n",
    "    input_A = Input(shape=(input_dim_A,))\n",
    "    x = input_A\n",
    "\n",
    "    input_B = Input(shape=(input_dim_B, 1))\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    num_conv_layers = hp.Int('num_conv_layers', 1, 4, default=1)\n",
    "    kernel_size = hp.Int('kernel_size', 51, 201, step=10, default=101)\n",
    "    stride_size = hp.Int('stride_size', 26, 101, step=5, default=51)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.05, default=0.1)\n",
    "    use_max_pooling = hp.Boolean('use_max_pooling', default=False)\n",
    "    num_filters = hp.Int('num_filters', 50, 100, step=10, default=50)\n",
    "\n",
    "    y = input_B\n",
    "    for i in range(num_conv_layers):\n",
    "        y = Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_size,\n",
    "            activation='relu',\n",
    "            padding='same')(y)\n",
    "        \n",
    "        # Ensure the input size is appropriate for max pooling\n",
    "        if use_max_pooling and y.shape[1] > 1:\n",
    "            y = MaxPooling1D(pool_size=2)(y)\n",
    "        \n",
    "        y = Dropout(dropout_rate)(y)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(4, activation=\"relu\", name='output_B')(y)\n",
    "\n",
    "    con = concatenate([x, y])\n",
    "\n",
    "    z = Dense(\n",
    "        hp.Int('dense', 4, 640, step=32, default=256),\n",
    "        activation='relu')(con)\n",
    "    z = Dropout(hp.Float('dropout-2', 0.0, 0.5, step=0.05, default=0.0))(z)\n",
    "\n",
    "    output = Dense(1, activation=\"linear\")(z)\n",
    "    model = Model(inputs=[input_A, input_B], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def train_and_optimize_model(tuner, data, nb_epoch, batch_size):\n",
    "    outputFilePath = 'Estimator'\n",
    "    checkpointer = ModelCheckpoint(filepath=outputFilePath, verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=7, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "\n",
    "    tuner.search([X_train_biological_data, X_train_wavenumbers], y_train,\n",
    "                 epochs=nb_epoch,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 validation_split=0.25,\n",
    "                 verbose=1,\n",
    "                 callbacks=[earlystop])\n",
    "\n",
    "    model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    return model, best_hp\n",
    "\n",
    "def final_training_pass(model, data, nb_epoch, batch_size):\n",
    "    outputFilePath = 'Estimator'\n",
    "    checkpointer = ModelCheckpoint(filepath=outputFilePath, verbose=1, save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "\n",
    "    history = model.fit([X_train_biological_data, X_train_wavenumbers], y_train,\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_split=0.25,\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpointer, earlystop]).history\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    X_test_biological_data = data.loc[data['sample'] == 'test', data.columns[4:8]]\n",
    "    X_test_wavenumbers = data.loc[data['sample'] == 'test', data.columns[8:]]\n",
    "    y_test = data.loc[data['sample'] == 'test', 'Age']\n",
    "\n",
    "    evaluation = model.evaluate([X_test_biological_data, X_test_wavenumbers], y_test)\n",
    "    preds = model.predict([X_test_biological_data, X_test_wavenumbers])\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    return evaluation, preds, r2\n",
    "\n",
    "def plot_predictions(y_test, preds):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, preds)\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')\n",
    "    lims = [-2.5, 5]\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    plt.plot(lims, lims)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_error(preds, y_test):\n",
    "    preds = np.array(preds).flatten()\n",
    "    y_test = y_test.to_numpy().flatten()\n",
    "    error = preds - y_test\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.hist(error, bins=20)\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_training_set(model, data, scaler_y):\n",
    "    X_train_biological_data = data.loc[data['sample'] == 'training', data.columns[4:8]]\n",
    "    X_train_wavenumbers = data.loc[data['sample'] == 'training', data.columns[8:]]\n",
    "    y_train = data.loc[data['sample'] == 'training', 'Age']\n",
    "    f_train = data.loc[data['sample'] == 'training', 'file_name']\n",
    "\n",
    "    y_train = np.array(y_train).reshape(-1, 1)\n",
    "    preds_t = model.predict([X_train_biological_data, X_train_wavenumbers])\n",
    "    \n",
    "    preds_t = preds_t.reshape(-1, 1)\n",
    "    y_train_reshaped = y_train.reshape(-1, 1)\n",
    "    \n",
    "    y_pr_transformed = scaler_y.inverse_transform(preds_t)\n",
    "    y_tr_transformed = scaler_y.inverse_transform(y_train_reshaped)\n",
    "\n",
    "    r_squared_tr = r2_score(y_tr_transformed, y_pr_transformed)\n",
    "    rmse_tr = sqrt(mean_squared_error(y_tr_transformed, y_pr_transformed))\n",
    "\n",
    "    y_tr_df = pd.DataFrame(y_tr_transformed, columns=['train'])\n",
    "    y_tr_df['pred'] = y_pr_transformed\n",
    "    y_tr_df['file'] = f_train.reset_index(drop=True)\n",
    "\n",
    "    y_tr_df.to_csv('./Output/Data/train_predictions.csv', index=False)\n",
    "\n",
    "    return r_squared_tr, rmse_tr, y_tr_df\n",
    "\n",
    "def plot_training_set(y_tr_transformed, y_pr_transformed):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.set_context(\"poster\")\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    p = sns.regplot(x=y_tr_transformed, y=y_pr_transformed, ci=None,\n",
    "                    scatter_kws={\"edgecolor\": 'b', 'linewidths': 2, \"alpha\": 0.5, \"s\": 150},\n",
    "                    line_kws={\"alpha\": 0.5, \"lw\": 4})\n",
    "    ax.plot([y_tr_transformed.min(), y_tr_transformed.max()], [y_tr_transformed.min(), y_tr_transformed.max()], 'k--', lw=2)\n",
    "\n",
    "    p.set(xlim=(-1, 24))\n",
    "    p.set(ylim=(-1, 24))\n",
    "    sns.despine()\n",
    "    plt.title('Training Set', fontsize=25)\n",
    "    plt.xlabel('Traditional Age (years)')\n",
    "    plt.ylabel('FT-NIR Age (years)')\n",
    "    plt.savefig('./Output/Figures/TrainingSet.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_set(y_test_transformed, y_pred_transformed):\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    p = sns.regplot(x=y_test_transformed, y=y_pred_transformed, ci=None,\n",
    "                    scatter_kws={\"edgecolor\": 'b', 'linewidths': 2, \"alpha\": 0.5, \"s\": 150},\n",
    "                    line_kws={\"alpha\": 0.5, \"lw\": 4})\n",
    "    ax.plot([y_test_transformed.min(), y_test_transformed.max()], [y_test_transformed.min(), y_test_transformed.max()], 'k--', lw=2)\n",
    "\n",
    "    p.set(xlim=(-1, 24))\n",
    "    p.set(ylim=(-1, 24))\n",
    "    sns.despine()\n",
    "    plt.title('Test Set', fontsize=25)\n",
    "    plt.xlabel('Traditional Age (years)')\n",
    "    plt.ylabel('FT-NIR Age (years)')\n",
    "    plt.savefig('./Output/Figures/TestSet.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_bland_altman(y_test_transformed, y_pred_transformed):\n",
    "    pyCompare.blandAltman(y_test_transformed.flatten(), y_pred_transformed.flatten(),\n",
    "                          limitOfAgreement=1.96, confidenceInterval=95,\n",
    "                          confidenceIntervalMethod='approximate',\n",
    "                          detrend=None, percentage=False,\n",
    "                          title='Bland-Altman Plot\\n',\n",
    "                          savePath='./Output/Figures/BlandAltman.png')\n",
    "\n",
    "def build_model_manual(input_dim_A, input_dim_B, num_conv_layers, kernel_size, stride_size, dropout_rate, use_max_pooling, num_filters, dense_units, dropout_rate_2):\n",
    "    input_A = Input(shape=(input_dim_A,))\n",
    "    x = input_A\n",
    "\n",
    "    input_B = Input(shape=(input_dim_B, 1))\n",
    "    y = input_B\n",
    "    for i in range(num_conv_layers):\n",
    "        y = Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride_size,\n",
    "            activation='relu',\n",
    "            padding='same')(y)\n",
    "        \n",
    "        if use_max_pooling and y.shape[1] > 1:\n",
    "            y = MaxPooling1D(pool_size=2)(y)\n",
    "        \n",
    "        y = Dropout(dropout_rate)(y)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(4, activation=\"relu\", name='output_B')(y)\n",
    "\n",
    "    con = concatenate([x, y])\n",
    "\n",
    "    z = Dense(dense_units, activation='relu')(con)\n",
    "    z = Dropout(dropout_rate_2)(z)\n",
    "\n",
    "    output = Dense(1, activation=\"linear\")(z)\n",
    "    model = Model(inputs=[input_A, input_B], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def InferenceMode(model_or_path, data, row_number, scaler_y=None, scaler_x=None):\n",
    "    if isinstance(model_or_path, str):\n",
    "        # Load model from disk\n",
    "        model = load_model(model_or_path)\n",
    "    else:\n",
    "        # Use the provided model object\n",
    "        model = model_or_path\n",
    "    \n",
    "    # Extract the specific row for inference\n",
    "    sample_data = data.iloc[row_number]\n",
    "    \n",
    "    # Extract biological and wavenumber data separately\n",
    "    biological_data = sample_data[data.columns[4:8]].values.reshape(1, -1)\n",
    "    wavenumber_data = sample_data[data.columns[8:]].values.reshape(1, -1, 1)\n",
    "    \n",
    "    # Apply the same scaling used during training\n",
    "    if scaler_x:\n",
    "        biological_data = scaler_x.transform(biological_data)\n",
    "        wavenumber_data = scaler_x.transform(wavenumber_data.reshape(1, -1)).reshape(1, -1, 1)\n",
    "    \n",
    "    # Run inference\n",
    "    prediction = model.predict([biological_data, wavenumber_data])\n",
    "    \n",
    "    # Inverse transform the prediction to original scale\n",
    "    if scaler_y:\n",
    "        prediction = scaler_y.inverse_transform(prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def TrainingModeWithHyperband(filepath, filter_CHOICE, scaling_CHOICE):\n",
    "    data = read_and_clean_data(filepath)\n",
    "    \n",
    "    data = preprocess_spectra(data, filter_type=filter_CHOICE)\n",
    "    \n",
    "    y_col = 'Age'\n",
    "    scaling_method = scaling_CHOICE  # 'minmax', 'standard', 'maxabs', 'robust', or 'normalize'\n",
    "    data, scaler_y = apply_scaling(data, scaling_method, y_col)\n",
    "\n",
    "    input_dim_A = data.columns[4:8].shape[0]\n",
    "    input_dim_B = data.columns[8:].shape[0]\n",
    "    \n",
    "    def model_builder(hp):\n",
    "        return build_model(hp, input_dim_A, input_dim_B)\n",
    "    \n",
    "    tuner = Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_loss',\n",
    "        max_epochs=1,\n",
    "        directory='Tuners',\n",
    "        project_name='mmcnn',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(tuner.search_space_summary())\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    model, best_hp = train_and_optimize_model(tuner, data, nb_epoch, batch_size)\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    history = final_training_pass(model, data, nb_epoch, batch_size)\n",
    "    \n",
    "    evaluation, preds, r2 = evaluate_model(model, data)\n",
    "    print(f\"Evaluation: {evaluation}, R2: {r2}\")\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    training_outputs = {\n",
    "        'trained_model': model,\n",
    "        'scaler' : scaler_y,\n",
    "        'training_history': history,\n",
    "        'evaluation': evaluation,\n",
    "        'predictions': preds,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "    \n",
    "    # top_5_models = tuner.get_best_models(num_models=5)\n",
    "    \n",
    "    additional_outputs = {\n",
    "        # 'top5models': top_5_models\n",
    "    }\n",
    "    \n",
    "    return training_outputs, additional_outputs\n",
    "\n",
    "def TrainingModeWithoutHyperband(filepath, filter_CHOICE, scaling_CHOICE, model_parameters):\n",
    "    if len(model_parameters) != 8:\n",
    "        raise ValueError(\"model_parameters must be a list of 8 values.\")\n",
    "    \n",
    "    num_conv_layers, kernel_size, stride_size, dropout_rate, use_max_pooling, num_filters, dense_units, dropout_rate_2 = model_parameters\n",
    "    \n",
    "    if not all(isinstance(param, (int, float, bool)) for param in model_parameters):\n",
    "        raise ValueError(\"All model parameters must be either int, float, or bool.\")\n",
    "    \n",
    "    data = read_and_clean_data(filepath)\n",
    "    \n",
    "    data = preprocess_spectra(data, filter_type=filter_CHOICE)\n",
    "    \n",
    "    y_col = 'Age'\n",
    "    scaling_method = scaling_CHOICE  # 'minmax', 'standard', 'maxabs', 'robust', or 'normalize'\n",
    "    data, scaler_y = apply_scaling(data, scaling_method, y_col)\n",
    "\n",
    "    input_dim_A = data.columns[4:8].shape[0]\n",
    "    input_dim_B = data.columns[8:].shape[0]\n",
    "\n",
    "    model = build_model_manual(\n",
    "        input_dim_A,\n",
    "        input_dim_B,\n",
    "        num_conv_layers,\n",
    "        kernel_size,\n",
    "        stride_size,\n",
    "        dropout_rate,\n",
    "        use_max_pooling,\n",
    "        num_filters,\n",
    "        dense_units,\n",
    "        dropout_rate_2\n",
    "    )\n",
    "    \n",
    "    nb_epoch = 1  # !@!\n",
    "    batch_size = 32\n",
    "    history = final_training_pass(model, data, nb_epoch, batch_size)\n",
    "    \n",
    "    evaluation, preds, r2 = evaluate_model(model, data)\n",
    "    print(f\"Evaluation: {evaluation}, R2: {r2}\")\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    training_outputs = {\n",
    "        'trained_model': model,\n",
    "        'scaler': scaler_y,\n",
    "        'training_history': history,\n",
    "        'evaluation': evaluation,\n",
    "        'predictions': preds,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "    \n",
    "    additional_outputs = {\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return training_outputs, additional_outputs\n",
    "\n",
    "\n",
    "def saveModel(model, path):\n",
    "    model.save(path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def saveModelWithMetadata(model, path, column_names, description):\n",
    "\n",
    "    # Save the Keras model\n",
    "    model.save(path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "    # Append metadata to the HDF5 file\n",
    "    with h5py.File(path, 'a') as f:\n",
    "        # Create a group for metadata if it doesn't exist\n",
    "        if 'metadata' not in f:\n",
    "            metadata_group = f.create_group('metadata')\n",
    "        else:\n",
    "            metadata_group = f['metadata']\n",
    "\n",
    "        # Add the column names list\n",
    "        if 'column_names' in metadata_group:\n",
    "            del metadata_group['column_names']  # Delete if exists to avoid duplication\n",
    "        metadata_group.create_dataset('column_names', data=column_names)\n",
    "        \n",
    "        # Add the description string\n",
    "        if 'description' in metadata_group:\n",
    "            del metadata_group['description']  # Delete if exists to avoid duplication\n",
    "        metadata_group.create_dataset('description', data=description)\n",
    "        \n",
    "    print(f\"Metadata added to {path}\")\n",
    "\n",
    "def loadModelMetadata(path):\n",
    "\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        metadata_group = f['metadata']\n",
    "        column_names = list(metadata_group['column_names'])\n",
    "        description = metadata_group['description'][()].decode('utf-8')\n",
    "\n",
    "    return column_names, description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe():\n",
    "    num_rows = 100\n",
    "\n",
    "    data = {\n",
    "        'filename': [f'file_{i}' for i in range(num_rows)],\n",
    "        'sample': ['training' if random.random() < 0.8 else 'test' for i in range(num_rows)],\n",
    "        'age': np.random.randint(1, 100, num_rows),\n",
    "        'weight': np.random.uniform(1.0, 100.0, num_rows),\n",
    "        'length': np.random.uniform(1.0, 100.0, num_rows),\n",
    "        'latitude': np.random.uniform(-90.0, 90.0, num_rows),\n",
    "        'longitude': np.random.uniform(-180.0, 180.0, num_rows),\n",
    "        'sex_M': np.random.choice([0, 1], num_rows),\n",
    "        'sex_F': np.random.choice([0, 1], num_rows),\n",
    "        'sex_immature': np.random.choice([0, 1], num_rows), \n",
    "    }\n",
    "\n",
    "    for i in range(1, 91):\n",
    "        data[f'var_{i}'] = np.random.randint(-1, 10, num_rows)\n",
    "\n",
    "    for i in range(1, 1001):\n",
    "        data[f'wavenumber_{i}'] = np.random.uniform(-1.0, 1.0, num_rows)\n",
    "\n",
    "    # data['metadata'] = [4000, 9000, 5] + [np.nan] * (num_rows - 3)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "sample_data_format = create_dataframe()\n",
    "\n",
    "sample_data_format\n",
    "\n",
    "sample_data_format.to_csv('Data/sample_data.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filename, sample, age, weight, length, latitude, longitude, sex_M, sex_F, sex_immature, var_1-90,\n",
    "# Wavenumbers: columns 101+ i.e. 101-1100\n",
    "# Last column: wavenumber metadata: start, end, step (rows 1-3) not anymore!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  file_name    sample   Age  latitude  length  \\\n",
      "0  WALLEYE_POLLOCK_157201603201_60411_OT1.0  training  10.0  53.97653   610.0   \n",
      "1  WALLEYE_POLLOCK_157201603201_60412_OT1.0  training   9.0  53.97653   611.0   \n",
      "2  WALLEYE_POLLOCK_157201603201_60414_OT1.0  training   6.0  53.97653   443.0   \n",
      "3  WALLEYE_POLLOCK_157201603201_60428_OT1.0  training   8.0  53.97653   479.0   \n",
      "4  WALLEYE_POLLOCK_157201603201_60450_OT1.0  training   6.0  53.97653   451.0   \n",
      "\n",
      "   gear_depth  gear_temp  wn11476.85064  wn11468.60577  wn11460.36091  ...  \\\n",
      "0      407.26       4.33       0.313066       0.313311       0.313633  ...   \n",
      "1      407.26       4.33       0.329684       0.330087       0.330593  ...   \n",
      "2      407.26       4.33       0.320618       0.321387       0.321874  ...   \n",
      "3      407.26       4.33       0.323654       0.323983       0.324420  ...   \n",
      "4      407.26       4.33       0.321214       0.321375       0.321607  ...   \n",
      "\n",
      "   wn4072.962798  wn4064.717934  wn4056.47307  wn4048.228206  wn4039.983342  \\\n",
      "0       1.316385       1.328887      1.339621       1.349055       1.357497   \n",
      "1       1.314472       1.330901      1.345002       1.357586       1.369454   \n",
      "2       1.170693       1.185670      1.198964       1.210918       1.221972   \n",
      "3       1.174009       1.189664      1.203431       1.215842       1.227446   \n",
      "4       1.122849       1.136776      1.149139       1.160242       1.170705   \n",
      "\n",
      "   wn4031.738478  wn4023.493614  wn4015.24875  wn4007.003886  wn3998.759022  \n",
      "0       1.365173       1.372687      1.380318       1.387681       1.394300  \n",
      "1       1.380857       1.391871      1.402634       1.413231       1.423495  \n",
      "2       1.232669       1.243479      1.254195       1.264426       1.274083  \n",
      "3       1.238731       1.250119      1.261706       1.273054       1.283520  \n",
      "4       1.181071       1.191509      1.201935       1.212171       1.221968  \n",
      "\n",
      "[5 rows x 915 columns]\n",
      "['file_name', 'sample', 'Age', 'latitude', 'length', 'gear_depth', 'gear_temp', 'wn11476.85064', 'wn11468.60577', 'wn11460.36091']\n",
      "Reloading Tuner from Tuners\\mmcnn\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 8\n",
      "num_conv_layers (Int)\n",
      "{'default': 1, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "kernel_size (Int)\n",
      "{'default': 101, 'conditions': [], 'min_value': 51, 'max_value': 201, 'step': 10, 'sampling': 'linear'}\n",
      "stride_size (Int)\n",
      "{'default': 51, 'conditions': [], 'min_value': 26, 'max_value': 101, 'step': 5, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.05, 'sampling': 'linear'}\n",
      "use_max_pooling (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "num_filters (Int)\n",
      "{'default': 50, 'conditions': [], 'min_value': 50, 'max_value': 100, 'step': 10, 'sampling': 'linear'}\n",
      "dense (Int)\n",
      "{'default': 256, 'conditions': [], 'min_value': 4, 'max_value': 640, 'step': 32, 'sampling': 'linear'}\n",
      "dropout-2 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': 'linear'}\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "157/161 [============================>.] - ETA: 0s - loss: 211.4631 - mse: 211.4631 - mae: 5.4351\n",
      "Epoch 1: val_loss improved from inf to 0.02269, saving model to Estimator\n",
      "INFO:tensorflow:Assets written to: Estimator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Estimator\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 4s 13ms/step - loss: 206.3309 - mse: 206.3309 - mae: 5.3073 - val_loss: 0.0227 - val_mse: 0.0227 - val_mae: 0.1243\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0277 - mse: 0.0277 - mae: 0.1322\n",
      "55/55 [==============================] - 0s 2ms/step\n",
      "Evaluation: [0.02767881192266941, 0.02767881192266941, 0.13219918310642242], R2: -0.35598078096565033\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 907, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 15, 70)               7140      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 15, 70)               0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1050)                 0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " output_B (Dense)            (None, 4)                    4204      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8)                    0         ['input_1[0][0]',             \n",
      "                                                                     'output_B[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 516)                  4644      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 516)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    517       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16505 (64.47 KB)\n",
      "Trainable params: 16505 (64.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "{'trained_model': <keras.src.engine.functional.Functional object at 0x0000020340FF7AD0>, 'scaler': MinMaxScaler(), 'training_history': {'loss': [206.33091735839844], 'mse': [206.33091735839844], 'mae': [5.307309150695801], 'val_loss': [0.022694574669003487], 'val_mse': [0.022694574669003487], 'val_mae': [0.12427078932523727]}, 'evaluation': [0.02767881192266941, 0.02767881192266941, 0.13219918310642242], 'predictions': array([[0.13199751],\n",
      "       [0.1309836 ],\n",
      "       [0.12727137],\n",
      "       ...,\n",
      "       [0.14652742],\n",
      "       [0.14294638],\n",
      "       [0.14254016]], dtype=float32), 'r2_score': -0.35598078096565033}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Training With Hyperband\n",
    "training_outputs_hyperband, additional_outputs_hyperband = TrainingModeWithHyperband(\n",
    "    filepath='./Data/AGP_MMCNN_BSsurvey_pollock2014to2018.csv',\n",
    "    filter_CHOICE='savgol',\n",
    "    scaling_CHOICE='minmax'\n",
    ")\n",
    "\n",
    "print(training_outputs_hyperband)\n",
    "print(additional_outputs_hyperband)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  file_name    sample   Age  latitude  length  \\\n",
      "0  WALLEYE_POLLOCK_157201603201_60411_OT1.0  training  10.0  53.97653   610.0   \n",
      "1  WALLEYE_POLLOCK_157201603201_60412_OT1.0  training   9.0  53.97653   611.0   \n",
      "2  WALLEYE_POLLOCK_157201603201_60414_OT1.0  training   6.0  53.97653   443.0   \n",
      "3  WALLEYE_POLLOCK_157201603201_60428_OT1.0  training   8.0  53.97653   479.0   \n",
      "4  WALLEYE_POLLOCK_157201603201_60450_OT1.0  training   6.0  53.97653   451.0   \n",
      "\n",
      "   gear_depth  gear_temp  wn11476.85064  wn11468.60577  wn11460.36091  ...  \\\n",
      "0      407.26       4.33       0.313066       0.313311       0.313633  ...   \n",
      "1      407.26       4.33       0.329684       0.330087       0.330593  ...   \n",
      "2      407.26       4.33       0.320618       0.321387       0.321874  ...   \n",
      "3      407.26       4.33       0.323654       0.323983       0.324420  ...   \n",
      "4      407.26       4.33       0.321214       0.321375       0.321607  ...   \n",
      "\n",
      "   wn4072.962798  wn4064.717934  wn4056.47307  wn4048.228206  wn4039.983342  \\\n",
      "0       1.316385       1.328887      1.339621       1.349055       1.357497   \n",
      "1       1.314472       1.330901      1.345002       1.357586       1.369454   \n",
      "2       1.170693       1.185670      1.198964       1.210918       1.221972   \n",
      "3       1.174009       1.189664      1.203431       1.215842       1.227446   \n",
      "4       1.122849       1.136776      1.149139       1.160242       1.170705   \n",
      "\n",
      "   wn4031.738478  wn4023.493614  wn4015.24875  wn4007.003886  wn3998.759022  \n",
      "0       1.365173       1.372687      1.380318       1.387681       1.394300  \n",
      "1       1.380857       1.391871      1.402634       1.413231       1.423495  \n",
      "2       1.232669       1.243479      1.254195       1.264426       1.274083  \n",
      "3       1.238731       1.250119      1.261706       1.273054       1.283520  \n",
      "4       1.181071       1.191509      1.201935       1.212171       1.221968  \n",
      "\n",
      "[5 rows x 915 columns]\n",
      "['file_name', 'sample', 'Age', 'latitude', 'length', 'gear_depth', 'gear_temp', 'wn11476.85064', 'wn11468.60577', 'wn11460.36091']\n",
      "159/161 [============================>.] - ETA: 0s - loss: 0.0114 - mse: 0.0114 - mae: 0.0776\n",
      "Epoch 1: val_loss improved from inf to 0.00332, saving model to Estimator\n",
      "INFO:tensorflow:Assets written to: Estimator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Estimator\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 4s 18ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0774 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0441\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0470\n",
      "55/55 [==============================] - 0s 3ms/step\n",
      "Evaluation: [0.004343908745795488, 0.004343908745795488, 0.0469832606613636], R2: 0.7871925515974116\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 907, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 18, 50)               5100      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 18, 50)               0         ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 1, 50)                252550    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 1, 50)                0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 50)                   0         ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " output_B (Dense)            (None, 4)                    204       ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 8)                    0         ['input_3[0][0]',             \n",
      " )                                                                   'output_B[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 256)                  2304      ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 256)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    257       ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 260415 (1017.25 KB)\n",
      "Trainable params: 260415 (1017.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "{'trained_model': <keras.src.engine.functional.Functional object at 0x00000203084B3810>, 'scaler': MinMaxScaler(), 'training_history': {'loss': [0.011317831464111805], 'mse': [0.011317831464111805], 'mae': [0.07739924639463425], 'val_loss': [0.0033195470459759235], 'val_mse': [0.0033195470459759235], 'val_mae': [0.04405832290649414]}, 'evaluation': [0.004343908745795488, 0.004343908745795488, 0.0469832606613636], 'predictions': array([[0.3364354 ],\n",
      "       [0.3044804 ],\n",
      "       [0.36184034],\n",
      "       ...,\n",
      "       [0.20975763],\n",
      "       [0.24888901],\n",
      "       [0.32038233]], dtype=float32), 'r2_score': 0.7871925515974116}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Training Without Hyperband\n",
    "\n",
    "training_outputs_manual, additional_outputs_manual = TrainingModeWithoutHyperband(\n",
    "    filepath='./Data/AGP_MMCNN_BSsurvey_pollock2014to2018.csv',\n",
    "    filter_CHOICE='savgol',\n",
    "    scaling_CHOICE='minmax',\n",
    "    model_parameters=[2, 101, 51, 0.1, False, 50, 256, 0.1]\n",
    ")\n",
    "\n",
    "# num_conv_layers, kernel_size, stride_size, dropout_rate, use_max_pooling, num_filters, dense_units, dropout_rate_2 \n",
    "\n",
    "\n",
    "print(training_outputs_manual)\n",
    "print(additional_outputs_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Zakariaie\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Saving Model\n",
    "\n",
    "model = training_outputs_hyperband['trained_model']\n",
    "saveModel(model, \"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model_with_metadata.h5\n",
      "Metadata added to my_model_with_metadata.h5\n"
     ]
    }
   ],
   "source": [
    "# Saving Model w/ Metadata\n",
    "\n",
    "column_names = ['col1', 'col2', 'col3']\n",
    "description = 'This model is trained on data columns col1, col2, col3'\n",
    "saveModelWithMetadata(model, 'my_model_with_metadata.h5', column_names, description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: [b'col1', b'col2', b'col3']\n",
      "Description: This model is trained on data columns col1, col2, col3\n"
     ]
    }
   ],
   "source": [
    "# Loading Model Metadata\n",
    "\n",
    "metadata_path = 'my_model_with_metadata.h5'\n",
    "column_names, description = loadModelMetadata(metadata_path)\n",
    "print(f\"Column Names: {column_names}\")\n",
    "print(f\"Description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
